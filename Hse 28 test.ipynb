{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHzvCoeWYcIAj140CNOnPj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HKMAEC1/Runcode/blob/main/Hse%2028%20test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGmv4ZSXYY3i",
        "outputId": "a0c461e3-f694-428b-dd95-8a2d1075f4c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today's date: 2023-09-27\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "import time\n",
        "from google.colab import files\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from datetime import date\n",
        "today = date.today()\n",
        "print(\"Today's date:\", today)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hse28_master = pd.read_csv(\"hse28_resmaster.csv\")\n",
        "hse28_data = pd.DataFrame()\n",
        "\n",
        "# Find the largest number in the \"ad_id\" column\n",
        "largest_ad_id = hse28_master[\"ad_id\"].max()\n",
        "\n",
        "ad_id_start = largest_ad_id + 1\n",
        "ad_id_end = ad_id_start + 20\n",
        "\n",
        "# Take the first 20 rows from the 'ad_id' column\n",
        "#ad_ids_column = hse28_master['ad_id'].iloc[:10]\n",
        "\n",
        "#for index, ad_id in enumerate(ad_ids_column):\n",
        "for index, ad_id in enumerate(hse28_master['ad_id']):\n",
        "    print(ad_id)\n",
        "    try:\n",
        "        url_get = \"https://www.28hse.com/en/buy/residential/property-\" + str(ad_id)\n",
        "\n",
        "        ####################################\n",
        "        ## Web scrapping by Beautiful Soup##\n",
        "        ####################################\n",
        "        source = requests.get(url_get).text\n",
        "        webpage = BeautifulSoup(source, 'lxml')\n",
        "\n",
        "        if 'Error' in webpage.find('div', class_=\"header\").text:\n",
        "            print(\"Property listing is not found\")\n",
        "            continue\n",
        "\n",
        "        # Extract segment (Residential, Office, etc)\n",
        "        segment = webpage.find('div', class_=\"ui small basic label\").text\n",
        "\n",
        "        # Extract created date and updated date\n",
        "        propertyDate = webpage.find('div', class_=\"propertyDate\").text.strip()\n",
        "\n",
        "        # Reg Ex\n",
        "        created_date = re.search(\"Created\\:(.+?) \\|\", propertyDate).group(1)\n",
        "        updated_date = re.search(\"Updated\\:(.+)\", propertyDate).group(1)\n",
        "\n",
        "        # Convert to date variable\n",
        "        created_date_d = datetime.strptime(created_date, '%Y-%m-%d').date()\n",
        "        updated_date_d = datetime.strptime(updated_date, '%Y-%m-%d').date()\n",
        "\n",
        "        # Convert to pd dataframe\n",
        "        prop_ad_df1 = dict(\n",
        "            ad_id=ad_id,\n",
        "            segment=segment,\n",
        "            scrape_date=today,\n",
        "            created_date=created_date_d,\n",
        "            updated_date=updated_date_d\n",
        "        )\n",
        "\n",
        "        prop_ad_df1 = pd.DataFrame([prop_ad_df1])\n",
        "\n",
        "        ######################################\n",
        "        # Web scraping by Pandas' read_html #\n",
        "        #######################################\n",
        "        pd_table = pd.read_html(url_get, skiprows=1)\n",
        "\n",
        "        prop_ad_df2 = pd_table[0].transpose()\n",
        "        prop_ad_df2.columns = prop_ad_df2.iloc[0]\n",
        "        prop_ad_df2 = prop_ad_df2.drop(0, axis=0).reset_index(drop=True)\n",
        "\n",
        "        ######################################\n",
        "        # Combine two tables into one df ##\n",
        "        #######################################\n",
        "        prop_ad_df3 = pd.concat([prop_ad_df1, prop_ad_df2], axis=1)\n",
        "        hse28_data = hse28_data.append(prop_ad_df3)\n",
        "\n",
        "        # Time pause for every 100 iterations\n",
        "        if (index + 1) % 100 == 0:\n",
        "            print(\"Pausing for 5 seconds...\")\n",
        "            time.sleep(5)\n",
        "\n",
        "    except:\n",
        "        print(\"Error during Web Scraping\")\n",
        "        pass\n",
        "\n",
        "for index, ad_id in enumerate(range(ad_id_start, ad_id_end + 1)):\n",
        "    print(ad_id)\n",
        "    try:\n",
        "        url_get = \"https://www.28hse.com/en/buy/residential/property-\" + str(ad_id)\n",
        "\n",
        "        ####################################\n",
        "        ## Web scrapping by Beautiful Soup##\n",
        "        ####################################\n",
        "        source = requests.get(url_get).text\n",
        "        webpage = BeautifulSoup(source, 'lxml')\n",
        "\n",
        "        if 'Error' in webpage.find('div', class_=\"header\").text:\n",
        "            print(\"Property listing is not found\")\n",
        "            continue\n",
        "\n",
        "        # Extract segment (Residential, Office, etc)\n",
        "        segment = webpage.find('div', class_=\"ui small basic label\").text\n",
        "\n",
        "        # Extract created date and updated date\n",
        "        propertyDate = webpage.find('div', class_=\"propertyDate\").text.strip()\n",
        "\n",
        "        # Reg Ex\n",
        "        created_date = re.search(\"Created\\:(.+?) \\|\", propertyDate).group(1)\n",
        "        updated_date = re.search(\"Updated\\:(.+)\", propertyDate).group(1)\n",
        "\n",
        "        # Convert to date variable\n",
        "        created_date_d = datetime.strptime(created_date, '%Y-%m-%d').date()\n",
        "        updated_date_d = datetime.strptime(updated_date, '%Y-%m-%d').date()\n",
        "\n",
        "        # Convert to pd dataframe\n",
        "        prop_ad_df1 = dict(\n",
        "            ad_id=ad_id,\n",
        "            segment=segment,\n",
        "            scrape_date=today,\n",
        "            created_date=created_date_d,\n",
        "            updated_date=updated_date_d\n",
        "        )\n",
        "\n",
        "        prop_ad_df1 = pd.DataFrame([prop_ad_df1])\n",
        "\n",
        "        ######################################\n",
        "        # Web scraping by Pandas' read_html #\n",
        "        #######################################\n",
        "        pd_table = pd.read_html(url_get, skiprows=1)\n",
        "\n",
        "        prop_ad_df2 = pd_table[0].transpose()\n",
        "        prop_ad_df2.columns = prop_ad_df2.iloc[0]\n",
        "        prop_ad_df2 = prop_ad_df2.drop(0, axis=0).reset_index(drop=True)\n",
        "\n",
        "        ######################################\n",
        "        # Combine two tables into one df ##\n",
        "        #######################################\n",
        "        prop_ad_df3 = pd.concat([prop_ad_df1, prop_ad_df2], axis=1)\n",
        "        hse28_data = hse28_data.append(prop_ad_df3)\n",
        "\n",
        "        # Time pause for every 100 iterations\n",
        "        if (index + 1) % 100 == 0:\n",
        "            print(\"Pausing for 5 seconds...\")\n",
        "            time.sleep(5)\n",
        "\n",
        "    except:\n",
        "        print(\"Error during Web Scraping\")\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "WYfkGuWeYbLB",
        "outputId": "d7b2ee1d-0d60-4eed-fba6-a93d91103e8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-7ff223ccb232>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    print(ad_id)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sub set of variables\n",
        "hse28_sub = hse28_data[['ad_id', 'scrape_date', 'segment', 'created_date', 'updated_date', 'Estate','Saleable Area', 'Gross Area', 'Sell Price', 'Monthly Rental']]\n",
        "\n",
        "# Date type\n",
        "hse28_sub['scrape_date']=pd.to_datetime(hse28_sub['scrape_date'])\n",
        "hse28_sub['created_date']=pd.to_datetime(hse28_sub['created_date'])\n",
        "hse28_sub['updated_date']=pd.to_datetime(hse28_sub['updated_date'])\n",
        "\n",
        "# Reg Ex to extract numbers\n",
        "hse28_sub['saleable_area_ft2']=hse28_sub['Saleable Area'].str.extract(r'^([^ft]+)')\n",
        "hse28_sub['saleable_area_ft2'] = hse28_sub['saleable_area_ft2'].str.replace(',','')\n",
        "hse28_sub['saleable_area_ft2']=pd.to_numeric(hse28_sub['saleable_area_ft2'])\n",
        "\n",
        "hse28_sub['gross_area_ft2']=hse28_sub['Gross Area'].str.extract(r'^([^ft]+)')\n",
        "hse28_sub['gross_area_ft2'] = hse28_sub['gross_area_ft2'].str.replace(',','')\n",
        "hse28_sub['gross_area_ft2']=pd.to_numeric(hse28_sub['gross_area_ft2'])\n",
        "\n",
        "#hse28_rental = hse28_sub.dropna(subset=['Monthly Rental'])\n",
        "#hse28_sell = hse28_sub.dropna(subset=['Sell Price'])\n",
        "\n",
        "try:\n",
        "    hse28_sub['price_hkdm'] = hse28_sub['Sell Price'].str.extract(r'(\\$.*(?= Millions))')\n",
        "except:\n",
        "    hse28_sub['price_hkdm'] = \"error\"\n",
        "\n",
        "try:\n",
        "    hse28_sub['price_hkdm'] = hse28_sub['price_hkdm'].str.replace('$', '', regex=True)\n",
        "except:\n",
        "    hse28_sub['price_hkdm'] = \"error\"\n",
        "\n",
        "try:\n",
        "    hse28_sub['price_hkdm'] = pd.to_numeric(hse28_sub['price_hkdm'], errors='coerce')\n",
        "except:\n",
        "    hse28_sub['price_hkdm'] = \"error\"\n",
        "\n",
        "try:\n",
        "    hse28_sub['rent_hkd'] = hse28_sub['Monthly Rental'].str.replace(',', '', regex=True)\n",
        "except:\n",
        "    hse28_sub['rent_hkd'] = \"error\"\n",
        "\n",
        "try:\n",
        "    hse28_sub['rent_hkd'] = hse28_sub['rent_hkd'].str.extract('(\\d+)')\n",
        "except:\n",
        "    hse28_sub['rent_hkd'] = \"error\"\n",
        "\n",
        "try:\n",
        "    hse28_sub['rent_hkd'] = pd.to_numeric(hse28_sub['rent_hkd'], errors='coerce')\n",
        "except:\n",
        "    hse28_sub['rent_hkd'] = \"error\"\n",
        "\n",
        "hse28_sub = hse28_sub.reset_index(drop=True)\n",
        "#hse28_sub"
      ],
      "metadata": {
        "id": "SIsZ7SiYYh3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the date as a string in the desired format\n",
        "date_string = today.strftime(\"%Y%m%d\")  # Change the format as per your preference\n",
        "\n",
        "# Generate the file name using the formatted date\n",
        "file_name = f\"hse28_{date_string}.csv\"\n",
        "\n",
        "# Save the DataFrame as a CSV with the generated file name\n",
        "file_path = f\"/content/{file_name}\"\n",
        "hse28_sub.to_csv(file_path, index=False)\n",
        "\n",
        "# Download the file from the \"Downloads\" folder\n",
        "files.download(file_path)"
      ],
      "metadata": {
        "id": "5GiXDs4JYlJj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}